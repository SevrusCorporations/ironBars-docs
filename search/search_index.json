{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IronBars Documentation","text":""},{"location":"#gsheet_load","title":"gsheet_load","text":"<p>Convert a Google Sheets, GitHub, or direct CSV link to a CSV-ready URL and optionally load as DataFrame(s).</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str | list</code> <p>URL(s) of CSV file(s).</p> required <code>as_df</code> <code>bool</code> <p>If True, return pandas DataFrame(s) instead of URL(s).</p> <code>False</code> <code>max_workers</code> <code>int | None</code> <p>Max threads for parallel downloading (only for list of URLs).</p> <code>None</code> <p>Returns:</p> Type Description <p>str | list | pd.DataFrame | list[pd.DataFrame]</p> Source code in <code>ironBars/ironSheets.py</code> <pre><code>def gsheet_load(url, as_df=False, max_workers=None):\n    \"\"\"\n    Convert a Google Sheets, GitHub, or direct CSV link to a CSV-ready URL and optionally load as DataFrame(s).\n\n    Parameters:\n        url (str | list): URL(s) of CSV file(s).\n        as_df (bool): If True, return pandas DataFrame(s) instead of URL(s).\n        max_workers (int | None): Max threads for parallel downloading (only for list of URLs).\n\n    Returns:\n        str | list | pd.DataFrame | list[pd.DataFrame]\n    \"\"\"\n\n    def convert_url(single_url):\n        parsed = urlparse(single_url)\n        netloc = parsed.netloc.lower()\n        path = parsed.path.lower()\n\n        # Google Sheets\n        if \"docs.google.com\" in netloc and \"/spreadsheets/\" in path:\n            if \"/edit\" in single_url:\n                return single_url.split(\"/edit\")[0] + \"/gviz/tq?tqx=out:csv\"\n            else:\n                return single_url\n\n        # GitHub\n        elif \"github.com\" in netloc:\n            if \"/blob/\" in single_url:\n                return single_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n            return single_url  # Already raw URL\n\n        # Direct CSV link\n        else:\n            return single_url\n\n    def fetch_csv(single_url):\n        csv_url = convert_url(single_url)\n        try:\n            return pd.read_csv(csv_url)\n        except Exception as e:\n            raise RuntimeError(f\"Failed to read CSV from {single_url}: {e}\")\n\n    # Handle single URL vs list\n    if isinstance(url, list):\n        if as_df:\n            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n                return list(executor.map(fetch_csv, url))\n        else:\n            return [convert_url(u) for u in url]\n    else:\n        converted = convert_url(url)\n        return pd.read_csv(converted) if as_df else converted\n</code></pre> <ul> <li>Load data from a Google Sheet into a pandas DataFrame.</li> </ul>"},{"location":"#gsheet_save","title":"gsheet_save","text":"<p>Save one or more DataFrames as CSV files.</p> <p>Parameters:</p> Name Type Description Default <code>data_frames</code> <code>DataFrame | list[DataFrame]</code> <p>Single dataframe or list of dataframes to save.</p> required <code>auto_name</code> <code>bool</code> <p>If True, generates filenames automatically using name_series + index (only for multiple frames).</p> <code>True</code> <code>name_series</code> <code>str | list</code> <p>Base name (str) or list of names (required if auto_name=False and multiple frames).</p> <code>'Sheet'</code> <code>save_dir</code> <code>str</code> <p>Directory to save CSV files. Defaults to current directory.</p> <code>'.'</code> <code>filename</code> <code>str | None</code> <p>Filename for single dataframe. Required if saving a single DataFrame.</p> <code>None</code> Source code in <code>ironBars/ironSheets.py</code> <pre><code>def gsheet_save(data_frames, auto_name=True, name_series=\"Sheet\", save_dir=\".\", filename=None):\n    \"\"\"\n    Save one or more DataFrames as CSV files.\n\n    Parameters:\n        data_frames (pd.DataFrame | list[pd.DataFrame]): Single dataframe or list of dataframes to save.\n        auto_name (bool): If True, generates filenames automatically using name_series + index (only for multiple frames).\n        name_series (str | list): Base name (str) or list of names (required if auto_name=False and multiple frames).\n        save_dir (str): Directory to save CSV files. Defaults to current directory.\n        filename (str | None): Filename for single dataframe. Required if saving a single DataFrame.\n    \"\"\"\n    # Normalize input to list\n    single_input = False\n    if isinstance(data_frames, pd.DataFrame):\n        data_frames = [data_frames]\n        single_input = True\n\n    assert data_frames, \"DATA FRAMES must not be empty!\"\n\n    # Ensure absolute save directory path\n    save_dir = os.path.abspath(save_dir)\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Handle single dataframe case\n    if single_input:\n        assert filename, \"Must provide 'filename' when saving a single DataFrame\"\n        if not filename.lower().endswith(\".csv\"):\n            filename += \".csv\"\n        fpath = os.path.join(save_dir, filename)\n        data_frames[0].to_csv(fpath, index=False)\n        return  # Nothing to return\n\n    # Handle multiple dataframes\n    if auto_name:\n        if isinstance(name_series, list):\n            raise RuntimeError(\"Auto Name -&gt; True but name_series is a list! Provide a string base name instead.\")\n    else:\n        if not isinstance(name_series, list):\n            raise RuntimeError(\"Auto Name -&gt; False. Provide a list of filenames as name_series.\")\n        if len(name_series) &lt; len(data_frames):\n            raise ValueError(\"Number of filenames provided is less than number of data frames.\")\n\n    for idx, frame in enumerate(data_frames):\n        if auto_name:\n            fname = f\"{name_series}{idx}.csv\"\n        else:\n            fname = f\"{name_series[idx]}.csv\"\n        fpath = os.path.join(save_dir, fname)\n        frame.to_csv(fpath, index=False)\n</code></pre> <ul> <li>Save a pandas DataFrame to a Google Sheet.</li> </ul>"},{"location":"#fill_nans","title":"fill_nans","text":"<p>Fill NaN values in a DataFrame column or all numeric columns using either 'perlin' noise or 'linear' regression.</p> <ul> <li>df : pandas.DataFrame     Input DataFrame containing numeric columns.</li> <li>column_name : str, optional     Column to fill. If None, all numeric columns will be filled.</li> <li>method : str, default \"perlin\"     Filling method: \"perlin\" for Perlin noise, \"linear\" for linear regression.</li> <li>seed : int, default 0     Seed or starting index for Perlin noise generation.</li> <li>scale_factor : float, default 0.1     Step size for Perlin noise generation.</li> <li>lim_min : float, default 0     Minimum limit for filled values.</li> <li>lim_max : float, default 100     Maximum limit for filled values.</li> </ul> <ul> <li>pandas.DataFrame     A copy of the original DataFrame with NaNs filled, respecting user-defined limits.</li> </ul> <p>Raises: - TypeError: If the specified column is not numeric. - ValueError: If the method is not \"perlin\" or \"linear\".</p> Source code in <code>ironBars/ironSheets.py</code> <pre><code>def fill_nans(df, column_name=None, method=\"perlin\", seed=0, scale_factor=0.1, lim_min=0, lim_max=100):\n    \"\"\"\n    Fill NaN values in a DataFrame column or all numeric columns using either 'perlin' noise or 'linear' regression.\n\n    Parameters:\n    - df : pandas.DataFrame\n        Input DataFrame containing numeric columns.\n    - column_name : str, optional\n        Column to fill. If None, all numeric columns will be filled.\n    - method : str, default \"perlin\"\n        Filling method: \"perlin\" for Perlin noise, \"linear\" for linear regression.\n    - seed : int, default 0\n        Seed or starting index for Perlin noise generation.\n    - scale_factor : float, default 0.1\n        Step size for Perlin noise generation.\n    - lim_min : float, default 0\n        Minimum limit for filled values.\n    - lim_max : float, default 100\n        Maximum limit for filled values.\n\n    Returns:\n    - pandas.DataFrame\n        A copy of the original DataFrame with NaNs filled, respecting user-defined limits.\n\n    Raises:\n    - TypeError: If the specified column is not numeric.\n    - ValueError: If the method is not \"perlin\" or \"linear\".\n    \"\"\"\n    df_copy = df.copy()\n\n    if column_name is not None:\n        if not np.issubdtype(df_copy[column_name].dtype, np.number):\n            raise TypeError(f\"Column '{column_name}' must be numeric.\")\n        cols_to_fill = [column_name]\n    else:\n        cols_to_fill = df_copy.select_dtypes(include=[np.number]).columns.tolist()\n\n    for col in cols_to_fill:\n        arr = df_copy[col].to_numpy(dtype=float)\n        nan_mask = np.isnan(arr)\n\n        if not nan_mask.any():\n            continue\n\n        if method == \"perlin\":\n            mean_val = np.nanmean(arr)\n            std_val = np.nanstd(arr)\n            indices = np.arange(len(arr))\n            noise_vals = np.array([pnoise1((i + seed) * scale_factor) for i in indices])\n            noise_scaled = noise_vals * 2 * std_val + mean_val\n            noise_scaled = np.clip(noise_scaled, lim_min, lim_max)\n            arr[nan_mask] = noise_scaled[nan_mask]\n\n        elif method == \"linear\":\n            indices_all = np.arange(len(arr)).reshape(-1, 1)\n            X_train = indices_all[~nan_mask]\n            y_train = arr[~nan_mask]\n            X_pred = indices_all[nan_mask]\n\n            model = LinearRegression()\n            model.fit(X_train, y_train)\n            arr[nan_mask] = model.predict(X_pred)\n            arr = np.clip(arr, lim_min, lim_max)\n\n        else:\n            raise ValueError(\"Method must be 'perlin' or 'linear'\")\n\n        df_copy[col] = arr\n\n    return df_copy\n</code></pre> <ul> <li>Fill missing values in a DataFrame using Perlin Noise and Linear Regression.</li> </ul>"}]}